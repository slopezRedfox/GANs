{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCGAN keras Imagenes 64 x 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jupyter notebook --NotebookApp.max_buffer_size=6000000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "#Se importan las bibliotecas. En caso de querer hacer este proceso directamente\n",
    "#en un ordenador se deben instalar las bilbiotecas que se indican a continuacion:\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import h5py\n",
    "import sklearn\n",
    "import glob\n",
    "import keras\n",
    "import time\n",
    "import imp\n",
    "import os\n",
    "import tensorflow as tf \n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "#Estas bibliotecas no son necesarias en su totalidad, solo se requieren ciertas\n",
    "#funciones\n",
    "from random import shuffle\n",
    "from skimage import io\n",
    "from collections import Counter\n",
    "from PIL import Image, ImageFont, ImageDraw, ImageEnhance\n",
    "\n",
    "from keras.datasets import mnist\n",
    "\n",
    "#Importe de funciones especificas de bibliotecas ya importadas\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Dropout, Activation, Flatten, Reshape\n",
    "from keras.layers import BatchNormalization, ZeroPadding2D, Reshape\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers.convolutional import UpSampling2D\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "from numpy import cov\n",
    "from numpy import trace\n",
    "from numpy import iscomplexobj\n",
    "from numpy import asarray\n",
    "from numpy.random import randint\n",
    "from scipy.linalg import sqrtm\n",
    "\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "from skimage.transform import resize\n",
    "from keras.datasets.mnist import load_data\n",
    "\n",
    "from scipy.stats import wasserstein_distance\n",
    "\n",
    "#lineas requeridas solo en jupyter notbook\n",
    "from IPython.display import clear_output\n",
    "%matplotlib notebook\n",
    "%matplotlib inline\n",
    "#%tensorflow_version 2.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se inicia definiendo el diccionario map_characters, este posee originalmente 18\n",
    "#personajes, si se agrega otro más se debe agregar al diccionario y colocarsele \n",
    "#el número consecutivo que corresponda\n",
    "\n",
    "map_characters = {0: 'trainingCoffee'}\n",
    "                  \n",
    "#Se define el numero de clases con la extensión del diccionario\n",
    "num_classes = len(map_characters)\n",
    "\n",
    "#Número máximo de imagenes que se usaran por personaje para entrenar\n",
    "pictures_per_class = 20000\n",
    "\n",
    "def load_pictures(test_size, pic_size,BGR):\n",
    "    \"\"\"\n",
    "    Load pictures from folders for characters from the map_characters dict and create a numpy dataset and \n",
    "    a numpy labels set. Pictures are re-sized into picture_size square.\n",
    "    :param BGR: boolean to use true color for the picture (RGB instead of BGR for plt)\n",
    "    :return: dataset, labels set\n",
    "    \"\"\"\n",
    "    pics = []    #Se crea la lista de imagenes de salida\n",
    "    labels = []  #Se crea la lista de etiquetas de salida\n",
    "\n",
    "    #k es el numero de la variable en el diccionario\n",
    "    #char es el nombre de la variable en el diccionario\n",
    "    # El if lo que hace es que recorra cada una de las variables del diccionario\n",
    "    for k, char in map_characters.items(): \n",
    "\n",
    "        #Esta linea deivuelve la lista de rutas de las imagenes de cada personaje del diccionario\n",
    "        # Ej: para char = 'abraham_grampa_simpson' regresa todas las rutas de imagen de la carpeta\n",
    "        # abraham_grampa_simpson que a su vez esta en la carpeta characters.\n",
    "        #Es solo una forma elegante y rapida de optener todas las rutas\n",
    "        pictures = [k for k in glob.glob('./source/trainingSet/%s/*' % char)]\n",
    "\n",
    "        #Se establece la cantidad de imagenes que se van a usar en cada clase\n",
    "        #Como maximo se usan 1176 imagenes, estas son tanto para train como para test\n",
    "        #Si el personaje en particular tiene menos de 1176 imagenes se usara la extension del personaje en cuestion\n",
    "        nb_pic = round(pictures_per_class/(1-test_size)) if round(pictures_per_class/(1-test_size))<len(pictures) else len(pictures)\n",
    "\n",
    "        #Se usa la funcion random para ordenar de manera aleatoria las imagenes de cada personaje\n",
    "        #Cabe detacar que pic es una ruta de acceso aleatoria de la lista de rutas de acceso en pictures\n",
    "        for pic in np.random.choice(pictures, nb_pic):\n",
    "\n",
    "            #Se lee la imagen de la ruta \"pic\"\n",
    "            a = cv2.imread(pic)\n",
    "            a = cv2.cvtColor(a, cv2.COLOR_BGR2RGB)\n",
    "            # a = cv2.imread(pic, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "            #Se hace un resize de las imagenes a pic_size x pic_size. Por defecoto\n",
    "            #este es 64x64. Esto concuerda con los pixeles de entrada de la red neuronal\n",
    "            a = cv2.resize(a, (pic_size,pic_size))\n",
    "\n",
    "            #Se agrega la imagen leida, convertida y escalada a la lista pics\n",
    "            pics.append(a)\n",
    "            #Se agrega la etiqueta numérica de la lista de etiquetas, esta etiqueta numerica \n",
    "            #concuerda con el número en el diccionario map_characters\n",
    "            labels.append(k)\n",
    "\n",
    "    #Se devuelve las listas pero antes se transforman a un formato de array con numpy\n",
    "    return np.array(pics), np.array(labels) \n",
    "\n",
    "def get_dataset(save=False, load=False, BGR=False, test_size=0.1, pic_size=64):\n",
    "    #Primero se llama a la funcion load_pictures, por defecto se deja estas en \n",
    "    #blanco y negro\n",
    "    #En esta funcion tambien se extraen los labels del archivo txt anotations\n",
    "    X, y = load_pictures(test_size, pic_size, BGR)\n",
    "\n",
    "    #Luego se pasa los labels a un formato numerico y en listas separadas,\n",
    "    #por ejemplo si el label es 2 to_categorical lo pasa a una lista [0 0 1 ... 0]\n",
    "    #El tamaño de la lista depende de cuantas clases se tenga, en este ejemplo son 18\n",
    "    y = keras.utils.to_categorical(y, num_classes)\n",
    "\n",
    "    #Se normaliza las imagenes para que los valores de cada banda esten en punto flotante\n",
    "    #y además se encuentren entre 0 y 1. Esto ayuda a la velocidad de entrenamiento           \n",
    "    X = X.astype('float32') / 255.\n",
    "    \n",
    "    #Se imprime las formas de cada tensor. Si se mantienen todas las variables \n",
    "    #por defecto estas deben ser (x,64,64,3) y (x,w), donde x es el numero de imagenes\n",
    "    #y w es la cantidad de clases en el diccionario\n",
    "    print(\"Train\", X.shape, y.shape)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "#Uso de funciones de lectura para dividir el set de imagnes en test y train\n",
    "X, y = get_dataset(save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hiper Parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dimensiones de imagen\n",
    "img_rows = 64\n",
    "img_cols = 64\n",
    "channels = 3\n",
    "latent_dim = 100\n",
    "img_shape = (img_rows, img_cols, channels)\n",
    "img_shape\n",
    "\n",
    "#Filtros y configuracion de red\n",
    "Filtros_Iniciales = 64  #Filtros iniciales en la primera capa de la generadora\n",
    "upsample_layers = 5     #Cantidad de layers que se tienen en la red generadora\n",
    "Kernels = 3             #Dimenciones de los filtros ejemplo: Kernels = 3 equivale a filtros 3x3\n",
    "\n",
    "#Optimizador Adam\n",
    "lr  = 0.0002\n",
    "b_1 = 0.5    #Beta 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuración de las Redes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la generación de imágenes se usará la aproximación GANs con dos redes convolucionales que compiten entre ellas, una se encarga de distinguir entre imágenes de café (Red Discriminadora) y la otra en tratar de engañar a la primera (Red Generadora)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Sistem_diagram](ReadMe_Images/Sistema.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arquitectura de Red Generadora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Gen_diagram](ReadMe_Images/Gen.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator():\n",
    "    noise_shape = (latent_dim,)\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(Filtros_Iniciales * (img_rows // (2 ** upsample_layers))  *  (img_cols // (2 ** upsample_layers)), activation=\"relu\", input_shape=noise_shape))\n",
    "    model.add(Reshape(((img_rows // (2 ** upsample_layers)),\n",
    "                       (img_cols // (2 ** upsample_layers)),\n",
    "                       Filtros_Iniciales)))\n",
    "    \n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "        \n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(1024, kernel_size=Kernels, padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    \n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(512, kernel_size=Kernels, padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    \n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(256, kernel_size=Kernels, padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(128, kernel_size=Kernels, padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    \n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(64, kernel_size=Kernels, padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    \n",
    "    model.add(Conv2D(channels, kernel_size=Kernels, padding=\"same\"))\n",
    "    model.add(Activation(\"tanh\"))\n",
    "    model.add(Reshape(img_shape))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    noise = Input(shape=(latent_dim,))\n",
    "    fake_img = model(noise)\n",
    "\n",
    "    return Model(noise, fake_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arquitectura de Red Discriminadora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Dis_diagram](ReadMe_Images/Dis.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator():\n",
    "\n",
    "    #Se crea primeramente un modelo \"Sequential\", este es el tipo de red neuronal\n",
    "    #existen otros tipos como las recursivas y las recurrentes.\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', input_shape=img_shape))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv2D(32, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv2D(64, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    #Finalmente se reduce a la cantidad de neuronas equivalentes a la cantidad de clases\n",
    "    #que se desean, en nuestro caso, la cantidad de personajes que estamos clasificando\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    img = Input(shape=img_shape)\n",
    "    validity = model(img)\n",
    "\n",
    "    return Model(img, validity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------\n",
    "itr_list = []\n",
    "acc_list = []\n",
    "d_loss_list = []\n",
    "g_loss_list = []\n",
    "fid_list = []\n",
    "wasserstein_distance_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train, epochs_inicial, epochs, batch_size=128, save_interval=50):\n",
    " \n",
    "    #La mitad del bach es imagenes falsas y la otra mitad imagenes reales\n",
    "    half_batch = int(batch_size / 2)\n",
    "\n",
    "    # Adversarial ground truths\n",
    "    valid = np.ones((half_batch, 1))\n",
    "    fake = np.zeros((half_batch, 1))\n",
    "    \n",
    "    for epoch in range(epochs_inicial, epochs):\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        \n",
    "        # Se seleccionan imagenes aleatorias para hacer la epoca\n",
    "        idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
    "        imgs = X_train[idx]\n",
    "\n",
    "        # El generador da un numero de imagenes sinteticas para la epoca\n",
    "        noise = np.random.normal(0, 1, (half_batch, latent_dim))\n",
    "        gen_imgs = generator.predict(noise)\n",
    "\n",
    "        #Se entrena con las imagenes reales\n",
    "        d_loss_real = discriminator.train_on_batch(imgs, valid)\n",
    "        \n",
    "        #Se entrena con las imagenes creadas como falsas\n",
    "        d_loss_fake = discriminator.train_on_batch(gen_imgs, fake)\n",
    "        \n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "        \n",
    "        # ---------------------\n",
    "        #  Train Generator\n",
    "        # ---------------------\n",
    "\n",
    "        # Se ingresa el ruido al generador y la idea es que\n",
    "        # el generador engañe al discriminador para que piense que es\n",
    "        # una imagen real, por eso ponemos que el ruido es valido\n",
    "        # Aqui solo se modifian los pesos del generador\n",
    "        g_loss = combined.train_on_batch(noise, valid)\n",
    "        \n",
    "        #------------------------------------------\n",
    "        # Calculo de Frechet Inception Distance\n",
    "        #------------------------------------------\n",
    "        \n",
    "        #Descomentar luego\n",
    "        # Se seleccionan imagenes aleatorias para hacer la epoca\n",
    "        idx = np.random.randint(0, X_train.shape[0], 128)\n",
    "        imgs = X_train[idx]\n",
    "\n",
    "        # El generador da un numero de imagenes sinteticas para la epoca\n",
    "        noise = np.random.normal(0, 1, (128, latent_dim))\n",
    "        gen_imgs = generator.predict(noise)\n",
    "        \n",
    "        # resize images\n",
    "        images1 = scale_images(imgs, (128,128,3))\n",
    "        images2 = scale_images(gen_imgs, (128,128,3))\n",
    "        \n",
    "        # pre-process images\n",
    "        images1 = preprocess_input(images1)\n",
    "        images2 = preprocess_input(images2)\n",
    "        \n",
    "        # calculate fid\n",
    "        fid = calculate_fid(model, images1, images2)\n",
    "        fid_list.append(fid)\n",
    "        \n",
    "        #---------------------------\n",
    "        # wasserstein_distance\n",
    "        #--------------------------\n",
    "        \n",
    "        wasserstein_distance_list.append(wasserstein_distance([d_loss[0]], [g_loss]))       \n",
    "        \n",
    "        #----------------------------------------------------------------------\n",
    "        #Se guarda los resultados en una lista\n",
    "        itr_list.append(epoch)\n",
    "        acc_list.append(100*d_loss[1])\n",
    "        d_loss_list.append(d_loss[0])\n",
    "        g_loss_list.append(g_loss)\n",
    "        \n",
    "        #----------------------------------------------------------------------\n",
    "        f, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, sharex='col', figsize=(13,7))\n",
    "        \n",
    "        ax1.plot(itr_list, d_loss_list, label = \"Discriminator Loss\")\n",
    "        ax1.plot(itr_list, g_loss_list, label = \"Generator Loss\")\n",
    "        ax1.legend()\n",
    "\n",
    "        #\n",
    "        ax2.plot(itr_list, wasserstein_distance_list, '-b', label = \"Wasserstein Distance\")\n",
    "        ax2.legend()\n",
    "        \n",
    "        #\n",
    "        ax3.plot(itr_list, fid_list, '-g', label = \"Frechet Inception Distance\")\n",
    "        ax3.legend()\n",
    "        \n",
    "        # Create two subplots and unpack the output array immediately\n",
    "        ax4.plot(itr_list, acc_list, '-r', label = \"Discriminator Accuracy\")\n",
    "        ax4.legend()\n",
    "        \n",
    "        plt.pause(0.0002)\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        # If at save interval => save generated image samples\n",
    "        if epoch % save_interval == 0:\n",
    "            #print (\"%d Discriminator_loss: %f acc.: %.2f%% Generator_loss: %f\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "            save_imgs(epoch)\n",
    "            \n",
    "        if epoch == epochs - 1:\n",
    "            #Cuendo termina el entrenamiento se guardan en un df\n",
    "            df = pd.DataFrame()\n",
    "            df['itr'] = itr_list\n",
    "            df['acc'] = acc_list\n",
    "            df['d_loss'] = d_loss_list\n",
    "            df['g_loss'] = g_loss_list\n",
    "            df['w_distance'] = wasserstein_distance_list\n",
    "            df['fid'] = fid_list\n",
    "            df.to_csv('results.csv', index=False)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculo del FID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://keras.io/api/applications/inceptionresnetv2/\n",
    "https://machinelearningmastery.com/how-to-implement-the-frechet-inception-distance-fid-from-scratch/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale an array of images to a new size\n",
    "def scale_images(images, new_shape):\n",
    "\timages_list = list()\n",
    "\tfor image in images:\n",
    "\t\t# resize with nearest neighbor interpolation\n",
    "\t\tnew_image = resize(image, new_shape, 0)\n",
    "\t\t# store\n",
    "\t\timages_list.append(new_image)\n",
    "\treturn asarray(images_list)\n",
    "\n",
    "# calculate frechet inception distance\n",
    "def calculate_fid(model, images1, images2):\n",
    "\t# calculate activations\n",
    "\tact1 = model.predict(images1)\n",
    "\tact2 = model.predict(images2)\n",
    "\t# calculate mean and covariance statistics\n",
    "\tmu1, sigma1 = act1.mean(axis=0), cov(act1, rowvar=False)\n",
    "\tmu2, sigma2 = act2.mean(axis=0), cov(act2, rowvar=False)\n",
    "\t# calculate sum squared difference between means\n",
    "\tssdiff = np.sum((mu1 - mu2)**2.0)\n",
    "\t# calculate sqrt of product between cov\n",
    "\tcovmean = sqrtm(sigma1.dot(sigma2))\n",
    "\t# check and correct imaginary numbers from sqrt\n",
    "\tif iscomplexobj(covmean):\n",
    "\t\tcovmean = covmean.real\n",
    "\t# calculate score\n",
    "\tfid = ssdiff + trace(sigma1 + sigma2 - 2.0 * covmean)\n",
    "\treturn fid\n",
    "\n",
    "# this could also be the output a different Keras model or layer\n",
    "input_tensor = Input(shape=(128, 128, 3))\n",
    "model = InceptionV3(input_tensor=input_tensor, weights='imagenet', include_top=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_imgs(epoch):\n",
    "    noise = np.random.normal(0, 1, (1, latent_dim))\n",
    "    gen_imgs = generator.predict(noise)\n",
    "\n",
    "    # Rescale images 0 - 1\n",
    "    gen_imgs = abs(gen_imgs)\n",
    "    gen_imgs = (abs(gen_imgs) * 255.).astype(int)\n",
    "\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(64/71.5, 64/71.5)\n",
    "    ax = plt.Axes(fig, [0., 0., 1., 1.])\n",
    "    ax.axis('off')\n",
    "    fig.add_axes(ax)\n",
    "    ax.imshow(gen_imgs[0])\n",
    "    fig.savefig(\"ResultadosCafe/mnist_%d.png\" % epoch)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento desde cero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se pone el optimizador junto con los hiper parametros\n",
    "optimizer = Adam(lr, b_1)\n",
    "\n",
    "# Build and compile the discriminator\n",
    "discriminator = build_discriminator()\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# Build the generator\n",
    "generator = build_generator()\n",
    "generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "# The generator takes noise as input and generates imgs\n",
    "z = Input(shape=(latent_dim,))\n",
    "img = generator(z)\n",
    "\n",
    "# For the combined model we will only train the generator\n",
    "discriminator.trainable = False\n",
    "\n",
    "# The discriminator takes generated images as input and determines validity\n",
    "valid = discriminator(img)\n",
    "\n",
    "# The combined model  (stacked generator and discriminator)\n",
    "# Trains the generator to fool the discriminator\n",
    "combined = Model(z, valid)\n",
    "combined.compile(loss='binary_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train!!!\n",
    "train(X, epochs_inicial=0, epochs=100001, batch_size=32, save_interval=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardado de datos!!!\n",
    "\n",
    "#Cuendo termina el entrenamiento se guardan en un df\n",
    "df = pd.DataFrame()\n",
    "df['itr'] = itr_list\n",
    "df['acc'] = acc_list\n",
    "df['d_loss'] = d_loss_list\n",
    "df['g_loss'] = g_loss_list\n",
    "df['w_distance'] = wasserstein_distance_list\n",
    "df['fid'] = fid_list\n",
    "df.to_csv('results.csv', index=False)    \n",
    "\n",
    "generator.save_weights('generator_64_64_weigths.h5')\n",
    "discriminator.save_weights('discriminator_64_64_weigths.h5')\n",
    "combined.save_weights('combined_64_64_weigths.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento desde un punto especifico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = pd.read_csv('Resultados_64_64/E1.0/results.csv')\n",
    "itr_list = list(Data.itr)\n",
    "acc_list = list(Data.acc)\n",
    "d_loss_list = list(Data.d_loss)\n",
    "g_loss_list = list(Data.g_loss)\n",
    "wasserstein_distance_list = list(Data.w_distance)\n",
    "fid_list = list(Data.fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imprime la ultima iteracion guardada\n",
    "itr_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "# Build and compile the discriminator\n",
    "discriminator = build_discriminator()\n",
    "discriminator.load_weights('Resultados_64_64/E1.0/discriminator_64_64_weigths.h5') #Load pesos\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# Build the generator\n",
    "generator = build_generator()\n",
    "generator.load_weights('Resultados_64_64/E1.0/generator_64_64_weigths.h5') #Load pesos\n",
    "generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "# The generator takes noise as input and generates imgs\n",
    "z = Input(shape=(latent_dim,))\n",
    "img = generator(z)\n",
    "\n",
    "# For the combined model we will only train the generator\n",
    "discriminator.trainable = False\n",
    "\n",
    "# The discriminator takes generated images as input and determines validity\n",
    "valid = discriminator(img)\n",
    "\n",
    "# The combined model  (stacked generator and discriminator)\n",
    "# Trains the generator to fool the discriminator\n",
    "combined = Model(z, valid)\n",
    "combined.compile(loss='binary_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(X, epochs_inicial= 40000, epochs=40002, batch_size=32, save_interval=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardado de datos!!!\n",
    "\n",
    "#Cuendo termina el entrenamiento se guardan en un df\n",
    "df = pd.DataFrame()\n",
    "df['itr'] = itr_list\n",
    "df['acc'] = acc_list\n",
    "df['d_loss'] = d_loss_list\n",
    "df['g_loss'] = g_loss_list\n",
    "df['w_distance'] = wasserstein_distance_list\n",
    "df['fid'] = fid_list\n",
    "df.to_csv('results.csv', index=False)    \n",
    "\n",
    "generator.save_weights('generator_64_64_weigths.h5')\n",
    "discriminator.save_weights('discriminator_64_64_weigths.h5')\n",
    "combined.save_weights('combined_64_64_weigths.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = pd.read_csv('Resultados_64_64/E1.0/results.csv')\n",
    "itr_list = list(Data.itr)\n",
    "acc_list = list(Data.acc)\n",
    "d_loss_list = list(Data.d_loss)\n",
    "g_loss_list = list(Data.g_loss)\n",
    "wasserstein_distance_list = list(Data.w_distance)\n",
    "fid_list = list(Data.fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, sharex='col', figsize=(13,7))\n",
    "\n",
    "ax1.plot(itr_list, d_loss_list, label = \"Discriminator Loss\")\n",
    "ax1.plot(itr_list, g_loss_list, label = \"Generator Loss\")\n",
    "ax1.legend()\n",
    "\n",
    "#\n",
    "ax2.plot(itr_list, wasserstein_distance_list, '-b', label = \"Wasserstein Distance\")\n",
    "ax2.legend()\n",
    "\n",
    "#\n",
    "ax3.plot(itr_list, fid_list, '-g', label = \"Frechet Inception Distance\")\n",
    "ax3.legend()\n",
    "\n",
    "# Create two subplots and unpack the output array immediately\n",
    "ax4.plot(itr_list, acc_list, '-r', label = \"Discriminator Accuracy\")\n",
    "ax4.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "\n",
    "path=\"ResultadosCafe/mnist_250.png\"\n",
    "display(Image.open(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "\n",
    "path=\"ResultadosCafe/mnist_79750.png\"\n",
    "display(Image.open(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test con pesos guardados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "new_model = build_generator()\n",
    "new_model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "new_model.load_weights('Resultados_64_64/EFinal/generator_64_64_weigths.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = np.random.normal(0, 1, (1,latent_dim))\n",
    "gen_imgs = new_model.predict(noise)\n",
    "\n",
    "gen_imgs2 = abs(gen_imgs)\n",
    "\n",
    "plt.imshow((gen_imgs2[0] * 255).astype(int))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = cv2.imread('./source/trainingSet/trainingCoffee/000004.jpg')\n",
    "a = cv2.cvtColor(a, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(a)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu2",
   "language": "python",
   "name": "gpu2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
